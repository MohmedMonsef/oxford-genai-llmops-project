[{"title": "LLM-Enhanced Data Management", "summary": "Machine learning (ML) techniques for optimizing data management problems have\nbeen extensively studied and widely deployed in recent five years. However\ntraditional ML methods have limitations on generalizability (adapting to\ndifferent scenarios) and inference ability (understanding the context).\nFortunately, large language models (LLMs) have shown high generalizability and\nhuman-competitive abilities in understanding context, which are promising for\ndata management tasks (e.g., database diagnosis, database tuning). However,\nexisting LLMs have several limitations: hallucination, high cost, and low\naccuracy for complicated tasks. To address these challenges, we design LLMDB,\nan LLM-enhanced data management paradigm which has generalizability and high\ninference ability while avoiding hallucination, reducing LLM cost, and\nachieving high accuracy. LLMDB embeds domain-specific knowledge to avoid\nhallucination by LLM fine-tuning and prompt engineering. LLMDB reduces the high\ncost of LLMs by vector databases which provide semantic search and caching\nabilities. LLMDB improves the task accuracy by LLM agent which provides\nmultiple-round inference and pipeline executions. We showcase three real-world\nscenarios that LLMDB can well support, including query rewrite, database\ndiagnosis and data analytics. We also summarize the open research challenges of\nLLMDB."}, {"title": "Scaling Efficient LLMs", "summary": "Trained LLMs are typically sparse in that most of the parameters are zero,\nraising questions on efficiency. In response, we inquire into efficient LLMs,\ni.e. those with the fewest parameters that achieve the desired accuracy on a\ntraining corpus. Specifically, we compare theoretical and empirical estimates\nfor training loss to obtain upper and lower bounds on the number of unique\nsequences in a natural training corpus as a function of its size. Our result\nimplies (1) to double the number of skills represented in a training corpus,\nthe corpus must scale more than four fold (2) for efficient LLMs, the number of\nparameters N and the size D of a natural training corpus scale as $N \\propto\nD^{0.44}$; (3) if the number of parameters of an LLM is smaller than the number\nof unique sequences in the training corpus, scaling up can uncover emergent\nskills."}, {"title": "VBART: The Turkish LLM", "summary": "We present VBART, the first Turkish sequence-to-sequence Large Language\nModels (LLMs) pre-trained on a large corpus from scratch. VBART are compact\nLLMs based on good ideas leveraged from BART and mBART models and come in two\nsizes, Large and XLarge. Fine-tuned VBART models surpass the prior\nstate-of-the-art results in abstractive text summarization, title generation,\ntext paraphrasing, question answering and question generation tasks. They allow\nfine-tuning for future text generation tasks and datasets, carving a new path\nfor Turkish Natural Language Processing (NLP) research. Our work shows that\nhaving a pre-trained LLM for Turkish outperforms up to 3x multilingual models,\nimproving existing results and providing efficient models for training and\ninference. Moreover, we show that our monolingual tokenizer is up to 11x more\nefficient than multilingual tokenizers. Last but not least, we introduce a\nmethod to enlarge an existing pre-trained LLM and question the relevancy of\nChinchilla Scaling Law to sequence-to-sequence masked language models. Our\nfine-tuned models, tokenizer and cleaned vngrs-web-corpus of 135 GB are\npublicly available at huggingface.co/vngrs-ai."}, {"title": "LLM-Oriented Retrieval Tuner", "summary": "Dense Retrieval (DR) is now considered as a promising tool to enhance the\nmemorization capacity of Large Language Models (LLM) such as GPT3 and GPT-4 by\nincorporating external memories. However, due to the paradigm discrepancy\nbetween text generation of LLM and DR, it is still an open challenge to\nintegrate the retrieval and generation tasks in a shared LLM. In this paper, we\npropose an efficient LLM-Oriented Retrieval Tuner, namely LMORT, which\ndecouples DR capacity from base LLM and non-invasively coordinates the\noptimally aligned and uniform layers of the LLM towards a unified DR space,\nachieving an efficient and effective DR without tuning the LLM itself. The\nextensive experiments on six BEIR datasets show that our approach could achieve\ncompetitive zero-shot retrieval performance compared to a range of strong DR\nmodels while maintaining the generation ability of LLM."}, {"title": "Auctions with LLM Summaries", "summary": "We study an auction setting in which bidders bid for placement of their\ncontent within a summary generated by a large language model (LLM), e.g., an ad\nauction in which the display is a summary paragraph of multiple ads. This\ngeneralizes the classic ad settings such as position auctions to an LLM\ngenerated setting, which allows us to handle general display formats. We\npropose a novel factorized framework in which an auction module and an LLM\nmodule work together via a prediction model to provide welfare maximizing\nsummary outputs in an incentive compatible manner. We provide a theoretical\nanalysis of this framework and synthetic experiments to demonstrate the\nfeasibility and validity of the system together with welfare comparisons."}, {"title": "Harmonic LLMs are Trustworthy", "summary": "We introduce an intuitive method to test the robustness (stability and\nexplainability) of any black-box LLM in real-time via its local deviation from\nharmoniticity, denoted as $\\gamma$. To the best of our knowledge this is the\nfirst completely model-agnostic and unsupervised method of measuring the\nrobustness of any given response from an LLM, based upon the model itself\nconforming to a purely mathematical standard. To show general application and\nimmediacy of results, we measure $\\gamma$ in 10 popular LLMs (ChatGPT,\nClaude-2.1, Claude3.0, GPT-4, GPT-4o, Smaug-72B, Mixtral-8x7B, Llama2-7B,\nMistral-7B and MPT-7B) across thousands of queries in three objective domains:\nWebQA, ProgrammingQA, and TruthfulQA. Across all models and domains tested,\nhuman annotation confirms that $\\gamma \\to 0$ indicates trustworthiness, and\nconversely searching higher values of $\\gamma$ easily exposes examples of\nhallucination, a fact that enables efficient adversarial prompt generation\nthrough stochastic gradient ascent in $\\gamma$. The low-$\\gamma$ leaders among\nthe models in the respective domains are GPT-4o, GPT-4, and Smaug-72B,\nproviding evidence that mid-size open-source models can win out against large\ncommercial models."}, {"title": "Exploiting LLM Quantization", "summary": "Quantization leverages lower-precision weights to reduce the memory usage of\nlarge language models (LLMs) and is a key technique for enabling their\ndeployment on commodity hardware. While LLM quantization's impact on utility\nhas been extensively explored, this work for the first time studies its adverse\neffects from a security perspective. We reveal that widely used quantization\nmethods can be exploited to produce a harmful quantized LLM, even though the\nfull-precision counterpart appears benign, potentially tricking users into\ndeploying the malicious quantized model. We demonstrate this threat using a\nthree-staged attack framework: (i) first, we obtain a malicious LLM through\nfine-tuning on an adversarial task; (ii) next, we quantize the malicious model\nand calculate constraints that characterize all full-precision models that map\nto the same quantized model; (iii) finally, using projected gradient descent,\nwe tune out the poisoned behavior from the full-precision model while ensuring\nthat its weights satisfy the constraints computed in step (ii). This procedure\nresults in an LLM that exhibits benign behavior in full precision but when\nquantized, it follows the adversarial behavior injected in step (i). We\nexperimentally demonstrate the feasibility and severity of such an attack\nacross three diverse scenarios: vulnerable code generation, content injection,\nand over-refusal attack. In practice, the adversary could host the resulting\nfull-precision model on an LLM community hub such as Hugging Face, exposing\nmillions of users to the threat of deploying its malicious quantized version on\ntheir devices."}, {"title": "Ranking LLMs by compression", "summary": "We conceptualize the process of understanding as information compression, and\npropose a method for ranking large language models (LLMs) based on lossless\ndata compression. We demonstrate the equivalence of compression length under\narithmetic coding with cumulative negative log probabilities when using a large\nlanguage model as a prior, that is, the pre-training phase of the model is\nessentially the process of learning the optimal coding length. At the same\ntime, the evaluation metric compression ratio can be obtained without actual\ncompression, which greatly saves overhead. In this paper, we use five large\nlanguage models as priors for compression, then compare their performance on\nchallenging natural language processing tasks, including sentence completion,\nquestion answering, and coreference resolution. Experimental results show that\ncompression ratio and model performance are positively correlated, so it can be\nused as a general metric to evaluate large language models."}, {"title": "LLMs can Schedule", "summary": "The job shop scheduling problem (JSSP) remains a significant hurdle in\noptimizing production processes. This challenge involves efficiently allocating\njobs to a limited number of machines while minimizing factors like total\nprocessing time or job delays. While recent advancements in artificial\nintelligence have yielded promising solutions, such as reinforcement learning\nand graph neural networks, this paper explores the potential of Large Language\nModels (LLMs) for JSSP. We introduce the very first supervised 120k dataset\nspecifically designed to train LLMs for JSSP. Surprisingly, our findings\ndemonstrate that LLM-based scheduling can achieve performance comparable to\nother neural approaches. Furthermore, we propose a sampling method that\nenhances the effectiveness of LLMs in tackling JSSP."}, {"title": "Efficient LLM Context Distillation", "summary": "This paper specifically investigates context distillation a method that\nextends the utility of task-specific examples by internalizing them, thus\naugmenting the example set accessible for model inference."}, {"title": "Encryption-Friendly LLM Architecture", "summary": "Large language models (LLMs) offer personalized responses based on user\ninteractions, but this use case raises serious privacy concerns. Homomorphic\nencryption (HE) is a cryptographic protocol supporting arithmetic computations\nin encrypted states and provides a potential solution for privacy-preserving\nmachine learning (PPML). However, the computational intensity of transformers\nposes challenges for applying HE to LLMs. In this work, we propose a modified\nHE-friendly transformer architecture with an emphasis on inference following\npersonalized (private) fine-tuning. Utilizing LoRA fine-tuning and Gaussian\nkernels, we achieve significant computational speedups -- 6.94x for fine-tuning\nand 2.3x for inference -- while maintaining performance comparable to plaintext\nmodels. Our findings provide a viable proof of concept for offering\nprivacy-preserving LLM services in areas where data protection is crucial. Our\ncode is available on GitHub."}, {"title": "Jailbreaking LLM-Controlled Robots", "summary": "The recent introduction of large language models (LLMs) has revolutionized\nthe field of robotics by enabling contextual reasoning and intuitive\nhuman-robot interaction in domains as varied as manipulation, locomotion, and\nself-driving vehicles. When viewed as a stand-alone technology, LLMs are known\nto be vulnerable to jailbreaking attacks, wherein malicious prompters elicit\nharmful text by bypassing LLM safety guardrails. To assess the risks of\ndeploying LLMs in robotics, in this paper, we introduce RoboPAIR, the first\nalgorithm designed to jailbreak LLM-controlled robots. Unlike existing, textual\nattacks on LLM chatbots, RoboPAIR elicits harmful physical actions from\nLLM-controlled robots, a phenomenon we experimentally demonstrate in three\nscenarios: (i) a white-box setting, wherein the attacker has full access to the\nNVIDIA Dolphins self-driving LLM, (ii) a gray-box setting, wherein the attacker\nhas partial access to a Clearpath Robotics Jackal UGV robot equipped with a\nGPT-4o planner, and (iii) a black-box setting, wherein the attacker has only\nquery access to the GPT-3.5-integrated Unitree Robotics Go2 robot dog. In each\nscenario and across three new datasets of harmful robotic actions, we\ndemonstrate that RoboPAIR, as well as several static baselines, finds\njailbreaks quickly and effectively, often achieving 100% attack success rates.\nOur results reveal, for the first time, that the risks of jailbroken LLMs\nextend far beyond text generation, given the distinct possibility that\njailbroken robots could cause physical damage in the real world. Indeed, our\nresults on the Unitree Go2 represent the first successful jailbreak of a\ndeployed commercial robotic system. Addressing this emerging vulnerability is\ncritical for ensuring the safe deployment of LLMs in robotics. Additional media\nis available at: https://robopair.org"}, {"title": "BrainTransformers: SNN-LLM", "summary": "This study introduces BrainTransformers, an innovative Large Language Model\n(LLM) implemented using Spiking Neural Networks (SNN). Our key contributions\ninclude: (1) designing SNN-compatible Transformer components such as SNNMatmul,\nSNNSoftmax, and SNNSiLU; (2) implementing an SNN approximation of the SiLU\nactivation function; and (3) developing a Synapsis module to simulate synaptic\nplasticity. Our 3-billion parameter model, BrainTransformers-3B-Chat,\ndemonstrates competitive performance across various benchmarks, including MMLU\n(63.2), BBH (54.1), ARC-C (54.3), and GSM8K (76.3), while potentially offering\nimproved energy efficiency and biological plausibility. The model employs a\nthree-stage training approach, including SNN-specific neuronal synaptic\nplasticity training. This research opens new avenues for brain-like AI systems\nin natural language processing and neuromorphic computing. Future work will\nfocus on hardware optimization, developing specialized SNN fine-tuning tools,\nand exploring practical applications in energy-efficient computing\nenvironments."}, {"title": "LLM Tree Search", "summary": "This project aims to investigate a novel sequence generation method inspired\nby the AlphaGo paradigm, adapting it for use with large language models (LLMs).\nThe proposed approach involves creating search trees of different possible\ncompletions and evaluating these completions based on model confidence. By\nconsidering various paths in the search tree and scoring them according to the\nmodel's confidence in each completion, we can generate diverse and high-quality\nsequences. This research explores the implementation of this paradigm by using\nconfidence as a proxy for response quality akin to beam search\n\\citep{vijayakumar2016diverse}. The primary goal of this paper is to outline\nthe paradigm and demonstrate its potential, rather than focusing on achieving\nperfect results. The paper will outline the reasons why we believe this\nparadigm has the potential to improve LLMs in the following manners: 1)\nincrease output quality, 2) decrease errors, 3) eliminate or reduce the\ncompound error problems, 4) generate diverse and creative completions, 5) allow\nfor iterative problem-solving, and 6) self-training. We expect this approach to\nyield a set of diverse and coherent sequences, offering insights into balancing\nexploration and exploitation in sequence generation. Potential applications\ninclude creative text generation tasks, such as storytelling and content\ncreation, as well as other natural language processing domains, like machine\ntranslation and automated summarization. The goal is that the model will be far\nmore effective as it will be able to consider many possible variations allowing\nit to find the ideal completion. This research aims to contribute to the\nunderstanding of effective search strategies in sequence generation and their\nimpact on generating high-quality, varied textual outputs."}, {"title": "Ontology Population using LLMs", "summary": "Knowledge graphs (KGs) are increasingly utilized for data integration,\nrepresentation, and visualization. While KG population is critical, it is often\ncostly, especially when data must be extracted from unstructured text in\nnatural language, which presents challenges, such as ambiguity and complex\ninterpretations. Large Language Models (LLMs) offer promising capabilities for\nsuch tasks, excelling in natural language understanding and content generation.\nHowever, their tendency to ``hallucinate'' can produce inaccurate outputs.\nDespite these limitations, LLMs offer rapid and scalable processing of natural\nlanguage data, and with prompt engineering and fine-tuning, they can\napproximate human-level performance in extracting and structuring data for KGs.\nThis study investigates LLM effectiveness for the KG population, focusing on\nthe Enslaved.org Hub Ontology. In this paper, we report that compared to the\nground truth, LLM's can extract ~90% of triples, when provided a modular\nontology as guidance in the prompts."}, {"title": "Densing Law of LLMs", "summary": "Large Language Models (LLMs) have emerged as a milestone in artificial\nintelligence, and their performance can improve as the model size increases.\nHowever, this scaling brings great challenges to training and inference\nefficiency, particularly for deploying LLMs in resource-constrained\nenvironments, and the scaling trend is becoming increasingly unsustainable.\nThis paper introduces the concept of ``\\textit{capacity density}'' as a new\nmetric to evaluate the quality of the LLMs across different scales and\ndescribes the trend of LLMs in terms of both effectiveness and efficiency. To\ncalculate the capacity density of a given target LLM, we first introduce a set\nof reference models and develop a scaling law to predict the downstream\nperformance of these reference models based on their parameter sizes. We then\ndefine the \\textit{effective parameter size} of the target LLM as the parameter\nsize required by a reference model to achieve equivalent performance, and\nformalize the capacity density as the ratio of the effective parameter size to\nthe actual parameter size of the target LLM. Capacity density provides a\nunified framework for assessing both model effectiveness and efficiency. Our\nfurther analysis of recent open-source base LLMs reveals an empirical law (the\ndensing law)that the capacity density of LLMs grows exponentially over time.\nMore specifically, using some widely used benchmarks for evaluation, the\ncapacity density of LLMs doubles approximately every three months. The law\nprovides new perspectives to guide future LLM development, emphasizing the\nimportance of improving capacity density to achieve optimal results with\nminimal computational overhead."}, {"title": "Asynchronous LLM Function Calling", "summary": "Large language models (LLMs) use function calls to interface with external\ntools and data source. However, the current approach to LLM function calling is\ninherently synchronous, where each call blocks LLM inference, limiting LLM\noperation and concurrent function execution. In this work, we propose AsyncLM,\na system for asynchronous LLM function calling. AsyncLM improves LLM's\noperational efficiency by enabling LLMs to generate and execute function calls\nconcurrently. Instead of waiting for each call's completion, AsyncLM introduces\nan interrupt mechanism to asynchronously notify the LLM in-flight when function\ncalls return. We design an in-context protocol for function calls and\ninterrupts, provide fine-tuning strategy to adapt LLMs to the interrupt\nsemantics, and implement these mechanisms efficiently on LLM inference process.\nWe demonstrate that AsyncLM can reduce end-to-end task completion latency from\n1.6x-5.4x compared to synchronous function calling on a set of benchmark tasks\nin the Berkeley function calling leaderboard (BFCL). Furthermore, we discuss\nhow interrupt mechanisms can be extended to enable novel human-LLM or LLM-LLM\ninteractions."}, {"title": "Multi-LLM Text Summarization", "summary": "In this work, we propose a Multi-LLM summarization framework, and investigate\ntwo different multi-LLM strategies including centralized and decentralized. Our\nmulti-LLM summarization framework has two fundamentally important steps at each\nround of conversation: generation and evaluation. These steps are different\ndepending on whether our multi-LLM decentralized summarization is used or\ncentralized. In both our multi-LLM decentralized and centralized strategies, we\nhave k different LLMs that generate diverse summaries of the text. However,\nduring evaluation, our multi-LLM centralized summarization approach leverages a\nsingle LLM to evaluate the summaries and select the best one whereas k LLMs are\nused for decentralized multi-LLM summarization. Overall, we find that our\nmulti-LLM summarization approaches significantly outperform the baselines that\nleverage only a single LLM by up to 3x. These results indicate the\neffectiveness of multi-LLM approaches for summarization."}, {"title": "Rerouting LLM Routers", "summary": "LLM routers aim to balance quality and cost of generation by classifying\nqueries and routing them to a cheaper or more expensive LLM depending on their\ncomplexity. Routers represent one type of what we call LLM control planes:\nsystems that orchestrate use of one or more LLMs. In this paper, we investigate\nrouters' adversarial robustness.\n  We first define LLM control plane integrity, i.e., robustness of LLM\norchestration to adversarial inputs, as a distinct problem in AI safety. Next,\nwe demonstrate that an adversary can generate query-independent token sequences\nwe call ``confounder gadgets'' that, when added to any query, cause LLM routers\nto send the query to a strong LLM.\n  Our quantitative evaluation shows that this attack is successful both in\nwhite-box and black-box settings against a variety of open-source and\ncommercial routers, and that confounding queries do not affect the quality of\nLLM responses. Finally, we demonstrate that gadgets can be effective while\nmaintaining low perplexity, thus perplexity-based filtering is not an effective\ndefense. We finish by investigating alternative defenses."}, {"title": "Transformer-Squared: Self-adaptive LLMs", "summary": "Self-adaptive large language models (LLMs) aim to solve the challenges posed\nby traditional fine-tuning methods, which are often computationally intensive\nand static in their ability to handle diverse tasks. We introduce\nTransformer-Squared, a novel self-adaptation framework that adapts LLMs for\nunseen tasks in real-time by selectively adjusting only the singular components\nof their weight matrices. During inference, Transformer-Squared employs a\ntwo-pass mechanism: first, a dispatch system identifies the task properties,\nand then task-specific 'expert' vectors, trained using reinforcement learning,\nare dynamically mixed to obtain targeted behavior for the incoming prompt. Our\nmethod consistently outperforms ubiquitous approaches such as LoRA, with fewer\nparameters and greater efficiency. Furthermore, Transformer-Squared\ndemonstrates versatility across different LLM architectures and modalities,\nincluding vision-language tasks. Transformer-Squared represents a significant\nleap forward, offering a scalable, efficient solution for enhancing the\nadaptability and task-specific performance of LLMs, paving the way for truly\ndynamic, self-organizing AI systems."}]